{"paragraphs":[{"title":"Decision Tree ","text":"// prepare data\nval dataWithoutHeader = spark.read.\n    option(\"inferSchema\", true).\n    option(\"header\", false).\n    csv(\"C:/Users/FKJMPY8/Downloads/covtype.data\")","user":"anonymous","dateUpdated":"2018-05-17T11:37:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dataWithoutHeader: org.apache.spark.sql.DataFrame = [value: string]\n"}]},"apps":[],"jobName":"paragraph_1526553611522_-1337642354","id":"20180517-104011_833842131","dateCreated":"2018-05-17T10:40:11+0000","dateStarted":"2018-05-17T11:35:04+0000","dateFinished":"2018-05-17T11:35:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20700"},{"text":"val colNames = Seq(\n        \"Elevation\", \"Aspect\", \"Slope\",\n        \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n        \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n        \"Horizontal_Distance_To_Fire_Points\") ++ \n            ((0 until 4).map(i => s\"Wilderness_Area_$i\")) ++\n            ((0 until 40).map(i => s\"Soil_Type_$i\")) ++ Seq(\"Cover_Type\")\n            \nval data = dataWithoutHeader.toDF(colNames:_*).withColumn(\"Cover_Type\", $\"Cover_Type\".cast(\"double\"))","user":"anonymous","dateUpdated":"2018-05-17T11:43:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526556922706_-1303681591","id":"20180517-113522_2027706173","dateCreated":"2018-05-17T11:35:22+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20701"},{"title":"A first decision tree","text":"//val Array(trainData, testData) = data.randomSplit(Array(0.9,0.1))\ntrainData.cache()\ntestData.cache()\n\n//all of input features need to be collected into one column, thus a vector\nimport org.apache.spark.ml.feature.VectorAssembler\nval inputCols = trainData.columns.filer(_ != \"Cover_Type\")\nval assembler = new VectorAssembler().\n    setInputCols(inputCols),\n    setOutputCol(\"featureVector\")\n    \nval assembledTrainData = assembler.transform(trainData)\nassembledTrainData.select(\"featureVector\").show(truncate = false)\n\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport scala.util.Random\n\nval classifier = new DecisionTreeClassifier().\n    setSeed(Random.nextLong()).\n    setLableCol(\"Cover_Type\").\n    setFeaturesCol(\"featureVector\").\n    setPredictionCol(\"prediction\")\n    \nval model = classifier.fit(assembledTrainData)\nprintln(model.toDebugString)\n\n// feature importance\nmodel.featureImportance.toArray.zip(inputCols).\n    sorted.reverse.foreach(println)\n    \n// to see what the model predicts on the training data\nval predicitons = model.transform(assembledTrainData)\npredictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(truncate = false)\n\n// evaluate\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval evaluator = new MulticlassClassificationEvaluator().\n    setLableCol(\"Cover_Type\").\n    setPredictionCol(\"prediction\")\n    \nevaluator.setMetricName(\"accuracy\").evaluate(predictions)\nevaluator.setMetricName(\"f1\").evaluate(predictions)\n\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n\nval predictionRDD = predictions.select(\"prediction\", \"Cover_Type\").as[(Double,Double)].rdd\n\nval multiclassMetrics = new MulticlassMetrics(predictionRDD)\nmulticlassMetrics.confusionMatrix\n\n//another way to get confusionmatrix\nval confusionMatrix = predictions.\n    groupBy(\"Cover_Type\").\n    pivot(\"prediction\", (1 to 7)).\n    count().\n    na.fill(0.0).\n    orderBy(\"Cover_Type\")\n    \nconfusionMatrix.show()\n\n// get guess classification accuracy\nimport org.apache.spark.sql.DataFrame\n\ndef classProbabilities(data: DataFrame): Array[Double] = {\n    val total = data.count()\n    data.groupBy(\"Cover_Type\").count().\n    orderBy(\"Cover_Type\").\n    select(\"count\").as[Double].\n    map(_ / total).collect()\n}\nval trainPriorProbabilities = classProbabilities(trainData)\nval testPriorProbabilities = classProbabilities(testData)\ntrainPriorProbabilities.zip(testPriorProbabilities).map {\n    case (trainProb, cvProb) => trainProb * cvProb\n}.sum","user":"anonymous","dateUpdated":"2018-05-17T13:04:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526557479140_-755032949","id":"20180517-114439_810632341","dateCreated":"2018-05-17T11:44:39+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20702"},{"text":"// Tuning Decision Trees\nimport org.apache.spark.ml.Pipeline\n\nval inputCols = trainData.columns.filter(_ != \"Cover_Type\")\nval assembler = new VectorAssembler().\n    setInputCols(inputCols).\n    setOutputCols(\"featureVector\")\n    \nval classifier = new DecisionTreeClassifier().\n    setSeed(Random.nextLong()).\n    setLableCol(\"Cover_Type\").\n    setFeaturesCol(\"featureVector\").\n    setPredictionCol(\"prediciton\")\n    \nval pipeline = new Pipeline().setStages(Array(assembler, classifier))\n\n// hyperparameter grid\nimport org.apache.spark.ml.tuning.ParamGridBuilder\n\nval paramGrid = new ParamGridBuilder().\n    addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n    addGrid(classifier.maxDepth, Seq(1,20)).\n    addGrid(classifier.maxBins, Seq(40,300)).\n    addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n    build()\n    \nval multiclassEval = new MulticlassClassificationEvaluator().\n    setLabelCol(\"Cover_Type\").\n    setPredictionCol(\"prediction\").\n    setMetricName(\"accuracy\")\n    \nimport org.apache.spark.ml.tuning.TrainValidationSplit\n\nval validator = new TrainValidationSplit().\n    setSeed(Random.nextLong()).\n    setEstimator(pipeline).\n    setEvaluator(multiclassEval).\n    setEstimatorParamMaps(paramGrid),\n    setTrainRatio(0.9)  // in tuning hyperparamters, 90% of trainData is used for train and 10% of trainData is used for test\n    \nval validatorModel = validator.fit(trainData)\n// to see the chosed best combination of hyperparameters\nimport org.apache.spark.ml.PipelineModel\nval bestModel = validatorModel.bestModel\nbestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap\n\n// to see the accuracy that each of the models achieved for each combination of hyperparameters\nval paramsAndMetrics = validatorModel.validationMetrics.zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n\nparamsAndMetrics.foreach { case (metric, params) =>\n    println(metrics)\n    println(params)\n    println()\n}\n// to see the accuracy of best model on train set and test dataset\nvalidatorModel.validationMetrics.max\nmulticlassEval.evaluate(bestModel.tranform(testData)) // bestModel is a complete pipeline\n","user":"anonymous","dateUpdated":"2018-05-17T14:50:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526562293016_1779846799","id":"20180517-130453_1441041730","dateCreated":"2018-05-17T13:04:53+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20703"},{"title":"Categorical features revisited","text":"// decision trees treat features individually, thus for categorical features encoded as several binary values, it is not reasonable\n// thus, make these several binary values into a vector, increase their relation as a categorical feature\nimport org.apache.spark.sql.functions._\ndef unencodeOneHot(data: DataFrame): DataFrame = {\n        val wildernessCols = (0 until 4).map(i => s\"Wilderness_Area_$i\").toArray\n        val wildernessAssembler = new VectorAssembler().\n            setInputCols(wildernessCols).\n            setOutputCol(\"wilderness\")\n            \n        val unhotUDF = udf((vec: Vector) => vec.toArray.indexOf(1.0).toDouble)\n        val withWilderness = wildernessAssembler.transform(data).drop(wildernessCols:_*).withColumn(\"wilderness\", unhotUDF($\"wilderness\"))\n        \n        val soilCols = (0 until 40).map(i => s\"Soil_Type_$i\").toArray\n        \n        val soilAssembler = new VectorAssembler().setInputCols(soilCols).setOutputCol(\"soil\")\n        \n        soilAssembler.transform(withWilderness).drop(soilCols:_*).withColumn(\"soil\", unhotUDF($\"soil\"))\n}","user":"anonymous","dateUpdated":"2018-05-18T09:56:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526636648328_-267096874","id":"20180518-094408_1583021739","dateCreated":"2018-05-18T09:44:08+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20704"},{"title":"Random Decision Forests","text":"import org.apache.spark.ml.classification.RandomForestClassifier\nval classifier = new RandomForestClassifier().\n    setSeed(Random.nextLong()).\n    setLabelCol(\"Cover_Type\").\n    setFeaturesCol(\"indexedVector\").\n    setPredictionCol(\"prediction\")\n    \nimport org.apache.spark.ml.classification.RandomForestClassificationModel\n\nval forestModel = bestModel.asInstanceOf[PipelineModel].\n    stages.last.asInstanceOf[RandomForestClassificationModel]\n    \nforestModel.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)\n\nbestModel.transform(unencTestData.drop(\"Cover_Type\")).select(\"prediction\").show()","user":"anonymous","dateUpdated":"2018-05-18T10:02:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526637408355_-922821203","id":"20180518-095648_1092846668","dateCreated":"2018-05-18T09:56:48+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20705"}],"name":"EE_YU/Advanced_Analytics_with_Spark/Chapter4","id":"2DEGJA35X","angularObjects":{"2CDBRM16S:shared_process":[],"2CGD9PJEV:shared_process":[],"2CEXJP4VC:shared_process":[],"2CEWFDUFC:shared_process":[],"2CCUD92DR:shared_process":[],"2CE5CMKN8:shared_process":[],"2CCT6VFHB:shared_process":[],"2CGDHC2BR:shared_process":[],"2CD5H5KA3:shared_process":[],"2CCFV6EWC:shared_process":[],"2CG8AUTUC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}