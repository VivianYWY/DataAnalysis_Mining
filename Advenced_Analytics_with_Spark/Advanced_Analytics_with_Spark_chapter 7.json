{"paragraphs":[{"title":"Analyzing co-occurence networks with GraphX","text":"// \"medline\" contains major tags of each record\n// get basic summary statistics about these tags in our dataset\nmedline.count()\nval topics = medline.flatMap(mesh => mesh).toDF(\"topic\")\ntopics.createOrReplaceTempView(\"topics\")\nval topicDist = spark.sql(\"\"\"\n    SELECT topic, COUNT(*) cnt\n    FROM topics\n    GROUP BY topic\n    ORDER BY cnt DESC\"\"\")\n\ntopicDist.count()\n\ntopicDist.createOrReplaceTempView(\"topic_dist\")\nspark.sql(\"\"\"\n    SELECT cnt, COUNT(*) dist\n    FROM topic_dist\n    GROUP BY cnt\n    ORDER BY dist DESC\n    LIMIT 10\"\"\").show()\n\n// generate the two-element sublists for each citation record\nval topicPairs = medline.flatMap(t => {\n    t.sorted.combinations(2)\n}),toDF(\"pairs\")\n\ntopicPairs.createOrReplaceTempView(\"topic_pairs\")\nval cooccurs = spark.sql(\"\"\"\n    SELECT pairs, COUNT(*) cnt\n    FROM topic_pairs\n    GROUP BY pairs\"\"\")\n    \ncooccurs.cache()\ncooccurs.count()\ncooccurs.createOrReplaceTempView(\"cooccurs\")\nspark.sql(\"\"\"\n    SELECT pairs, cnt\n    FROM cooccurs\n    ORDER BY cnt DESC\n    LIMIT 10\"\"\").collect().foreach(println)\n","user":"anonymous","dateUpdated":"2018-05-24T11:28:36+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526987449943_983928765","id":"20180522-111049_192667993","dateCreated":"2018-05-22T11:10:49+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:246"},{"title":"Constructing Co-occurrence Network with GraphX","text":"import java.nio.chartset.StandardCharsets\nimport java.security.MessageDigest\n\ndef hashId(str: String): Long = {\n    val bytes = MessageDigest.getInstance(\"MD5\").digest(str.getBytes(StandardCharsets.UTF_8))\n    (bytes(0) & 0xFFL) | ((bytes(1) & 0xFFL) << 8) | ((bytes(2) & 0xFFL) << 16) | ((bytes(3) & 0xFFL) << 24)| ((bytes(4) & 0xFFL) << 32)| ((bytes(5) & 0xFFL) << 40)| ((bytes(6) & 0xFFL) << 48)| ((bytes(7) & 0xFFL) << 56)\n}\n\nimport org.apache.spark.sql.Row\nval vertices = topics.map{ case Row(topic: String) => (hashId(topic), topic) }.toDF(\"hash\",\"topic\") \nval uniqueHashes = vertices.agg(countDistinct(\"hash\")).take(1)\n\nimport org.apache.spark.graphx._\nval edges = cooccurs.map { case Row(topics: Seq[_], cnt:Long) =>\n    val ids = topics.map(_.toString).map(hashId).sorted\n    Edge(ids(0), ids(1), cnt)\n}\n\nval vertexRDD = vertices.rdd.map{\n    case Row(hash: Long, topic: String) => (hash, topic)\n}\nval topicGraph = Graph(vertexRDD, edges.rdd) // graph will automatically deletre the duplicated vertexes, but not for edges\ntopicGraph.cache()\n","user":"anonymous","dateUpdated":"2018-05-24T11:23:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526987449697_1261717471","id":"20180522-111049_612930059","dateCreated":"2018-05-22T11:10:49+0000","dateStarted":"2018-05-23T12:15:01+0000","dateFinished":"2018-05-23T12:15:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"title":"understanding the structure of networks","text":"// connected components\nval connectedComponentGraph = topicGraph.connectedComponents()\nval componentDF = connectedComponentGraph.vertices.toDF(\"vid\", \"cid\")\nval componentCounts = componentDF.groupBy(\"cid\").count()\ncomponentCounts.count()\ncomponentCounts.orderBy(desc(\"count\")).show()\n\nval topicComponentDF = topicGraph.vertices.innerJoin(connectedComponentGraph.vertices) {\n    (topicId, name, componentId) => (name, componentId.toLong)\n}.toDF(\"topic\", \"cid\")\n\ntopicComponentDF.where(\"cid\" = -2062883918534425492).show()\nval campy = spark.sql.(\"\"\"\n    SELECT *\n    FROM topic_dist\n    WHERE topic LIKE '%ampylobacter%'\"\"\")\ncampy.show()\n\n// degree distribution\nval degrees: VertexRDD[Int] = topicGraph.degrees.cache()\ndegrees.map(_._2).stats()\n\nval namesAndDegrees = degrees.innerJoin(topicGraph.vertices){\n    (topicId, degree, name) => (name, degree.toInt)\n}.values.toDF(\"topic\",\"degree\")\n\nnamesAndDegrees.orderBy(desc(\"degree\")).show()\n\n// filter noisy edges\nval T = medline.count()\nval topicDistRdd = topicDist.map{\n    case Row(topic:String, cnt:Long) => (hashId(topic), cnt)\n}.rdd\n\nval topicDistGraph = Graph(topicDistRdd, topicGraph.edges)\n\nval chiSq(YY: Long, YB: Long, YA: Long, T:Long): Double = {\n    val NB = T - YB\n    val NY = T - YA\n    val YN = YA - YY\n    val NY = YB - YY\n    val NN = T - NY - YN - YY\n    val inner = math.abs(YY * NN - YN * NY) - T / 2.0\n    T * math.pow(inner,2) / (YA * NA * YB * NB)\n}\n\nval chiSquaredGraph = topicDistGraph.mapTriplets(triplet => {\n    chiSq(triplet.attr, triplet.srcAttr, triplet.dstAttr, T)\n})\n\nchiSquaredGraph.edges.map(x => x.attr).stats()\n\nval interesting = chiSquaredGraph.subgraph(triplet => triplet.attr > 19.5)\ninteresting.edges.count\nval interestingComponentGraph = interesting.connectedComponents()\nval icDF = interestingComponentGraph.vertices.toDF(\"vid\",\"cid\")\nval icCountDF = icDF.groupBy(\"cid\").count()\nicCountDF.count()\nicCountDF.orderBy(desc(\"count\")).show()\n\nval interestingDegrees = interesting.degrees.cache()\ninterestingDegrees.map(_._2).stats()\n\ninterestingDegrees.innerJoin(topicGraph.vertices){\n    (topicId, degree, name) => (name, degree)\n}.values.toDF(\"topic\",\"degree\").orderBy(desc(\"degree\")).show()","user":"anonymous","dateUpdated":"2018-05-24T12:03:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526987449458_1170531982","id":"20180522-111049_2004459227","dateCreated":"2018-05-22T11:10:49+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"title":"small-world networks","text":"// Cliques and clustering coefficients\nval triCountGraph = interesting.triangleCount()\ntriCountGraph.vertices.map( x => x._2).stats()\n\nval maxTrisGraph = interestingDegrees.mapValues(d => d * (d - 1)/ 2.0)\nval clusterCoef = triCountGraph.vertices.innerJoin(maxTrisGraph){\n    (vertexId, triCount, maxTris) => {\n        if (maxTris == 0) 0 else triCount/maxTris\n    }\n}\n\nclusterCoef.map(_._2).sum()/interesting.vertices.count()\n\n// Average path length\ndef mergeMaps(m1: Map[VertexId, Int], m2: Map[VertexId, Int]): Map[VertexId, Int] = {\n    def minThatExists(k: VertexId): Int = {\n        math.min(m1.getOrElse(k, Int.MaxValue), m2.getOrElse(k, Int.MaxValue))\n    }\n    \n    (m1.keySet ++ m2.keySet).map{\n        k => (k, minThatExists(k))\n    }.toMap\n}\n\ndef update(id: VertexId, state: Map[VertexId, Int], msg: Map[VertexId, Int]) = {\n    mergeMaps(state, msg)\n}\n\n","user":"anonymous","dateUpdated":"2018-05-24T14:02:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526987285907_-997991085","id":"20180522-110805_1081147151","dateCreated":"2018-05-22T11:08:05+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:249"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526985635284_-1756842322","id":"20180522-104035_1991605362","dateCreated":"2018-05-22T10:40:35+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527059583737_-923948103","id":"20180523-071303_1544638951","dateCreated":"2018-05-23T07:13:03+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:251"}],"name":"/EE_YU/Advanced_Analytics_with_Spark/chapter 7","id":"2DDUC892G","angularObjects":{"2CDBRM16S:shared_process":[],"2CGD9PJEV:shared_process":[],"2CEXJP4VC:shared_process":[],"2CEWFDUFC:shared_process":[],"2CCUD92DR:shared_process":[],"2CE5CMKN8:shared_process":[],"2CCT6VFHB:shared_process":[],"2CGDHC2BR:shared_process":[],"2CD5H5KA3:shared_process":[],"2CCFV6EWC:shared_process":[],"2CG8AUTUC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}