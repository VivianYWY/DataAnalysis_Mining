{"paragraphs":[{"title":"Geospatial and Temporal data analysis on New York city taxi trip data","text":"// define our own RichGeometry class that extends the Esri Geometry object with some useful helper methods:\nimport com.esri.core.geometry.Geometry\nimport com.esri.core.geometry.GeometryEngine\nimport com.esri.core.geometry.SpatialReference\n\nclass RichGeometry(val geometry: Geometry, val spatialReference: SpatialReference = SpatialReference.create(4326)) {\n    def area2D() = geometry.calculateArea2D()\n    \n    def contains(other: Geometry): Boolean = {\n        GeometryEngine.contains(geometry, other, spatialReference)\n    }\n    \n    def distance(other: Geometry): Double = {\n        GeometryEngine.distance(geometry, other, spatialReference)\n    }\n}.\n\nobject RichGeometry {\n    implicit def wrapRichGeo(g: Geometry) = {\n        new RichGeometry(g)\n    }\n}\n\n// define case classes to represent GeoJSON objects including feature (geometry, properties) and featurecollection\nimport spray.json.JsValue\n\ncase class Feature(val id: Option[JsValue], val properties: Map[String, JsValue], val geometry: RichGeometry){\n    def apply(property: String) = properties(property)\n    def get(property: String) = properties.get(property)\n}\n\ncase class FeatureCollection(features: Array[Feature]) extends IndexedSeq[Feature]{\n    def apply(index: Int) = features(index)\n    def length = features.length\n}\n\nimplicit object FeatureJsonFormat extends RootJsonFormat[Feature]{\n    def write(f: Feature) = {\n        val buf = scala.collection.mutable.ArrayBuffer(\"type\" -> JsString(\"Feature\"), \"properties\" -> JsObject(f.properties), \"geometry\"-> f.geometry.toJson)\n        f.id.foreach(v => {buf += \"id\" -> v})\n        JsObject(buf.toMap)\n    }\n    \n    def read(value: JsValue) = {\n        val jso = value.asJsObject\n        val id = jso.fields.get(\"id\")\n        val properties = jso.fields(\"properties\").asJsObject.fields\n        val geometry = jso.fields(\"geometry\").convertTo[RichGeometry]\n        Feature(id, properties, geometry)\n    }\n}\n\n// prepare the taxi trip data\nval taxiRaw = spark.read.option(\"header\", \"true\").csv(\"taxidata\")\ntaxiRaw.show()\n\ncase class Trip(\n    license: String,\n    pickupTime: Long,\n    dropoffTime: Long,\n    pickupX: Double,\n    pickupY: Double,\n    dropoffX: Double,\n    dropoffY: Double\n    )\n\nclass RichRow(row: org.apache.spark.sql.Row){\n    def getAs[T](field: String): Option[T] = {\n        if (row.isNullAt(row.fieldIndex(field))){\n            None\n        } else {\n            Some(row.getAs[T](field))\n        }\n    }\n}\n\ndef parseTaxiTime(rr: RichRow, timeField: String): Long = {\n    val formatter = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.ENGLISH)\n    val optDt = rr.getAs[String](timeField)\n    optDt.map(dt => formatter.parse(dt).getTime).getOrElse(0L)\n}\n\ndef parseTaxiLoc(rr: RichRow, locField: String): Double = {\n    rr.getAs[String](locField).map(_.toDouble).getOrElse(0.0)\n}\n\ndef parse(row: org.apache.spark.sql.Row): Trip = {\n    val rr = new RichRow(row)\n    Trip(\n        license = rr.getAs[String](\"hack_license\").orNull,\n        pickupTime = parseTaxiTime(rr, \"pickup_datetime\"),\n        dropoffTime = parseTaxiTime(rr, \"dropoff_datetime\"),\n        pickupX = parseTaxiLoc(rr, \"pickup_longitude\"),\n        pickupY = parseTaxiLoc(rr, \"pickup_latitude\"),\n        dropoffX = parseTaxiLoc(rr, \"dropoff_longitude\"),\n        dropoffY = parseTaxiLoc(rr,\"dropoff_latitude\"))\n}\n\ndef safe[S, T](f: S => T): S => Either[T, (S, Exception)] = {\n    new Function[S, Either[T, (S, Exception)]] with Serializable {\n        def apply(s: S): Either[T, (S, Exception)] = {\n            try{\n                Left(f(s))\n            }catch {\n                case e: Exception => Right((s,e))\n            }\n        }\n    }\n}\n\nval safeParse = safe(parse)\nval taxiParsed = taxiRaw.rdd.map(safeParse)\ntaxiParsed.map(_.isLeft).countByValue().foreach(println)\n\nval taxiGood = taxiParsed.map(_.left.get).toDS\ntaxiGood.cache()\n\nval hours = (pickup: Long, dropoff: Long) => {\n    TimeUnit.HOURS.convert(dropoff-pickup, TimeUnit.MILLISECONDS)\n}\nimport org.apache.spark.sql.functions.udf\nval hoursUDF = udf(hours)\ntaxiGood.groupBy(hoursUDF($\"pickupTime\", $\"dropoffTime\").as(\"h\")).count().sort(\"h\").show()\n\ntaxiGood.where(hoursUDF($\"pickupTime\", $\"dropoffTime\") < 0).collect().foreach(println)\nspark.udf.register(\"hours\", hours)\nval taxiClean = taxiGood.where(\"hours(pickupTime, dropoffTime) BETWEEN 0 AND 3\")\n\n// geospatial analysis\nval geojson = scala.io.Source.fromFile(\"nyc-boroughs.geojson\").mkString\nimport com.cloudera.datascience.geotime._\nimport GeoJsonProtocol._\nimport spray.json._\n\nval features = geojson.parseJson.convertTo[FeatureCollection]\n// to test if the function can correctly identify which borough a point belongs to\nimport com.esri.core.geometry.Point\nval p = new Point(-73.3333,40.4444)\nval borough = features.find(f => f.geometry.contains(p))\n\n// consider the \"find\" efficiency, put the most often borough and largest polygons\nval areaSortedFeatures = features.sortBy(f => {\n    val borough = f(\"boroughCode\").convertTo[Int]\n    (borough, -f.geometry.area2D())\n})\nval bFeatures = sc.broadcast(areaSortedFeatures)\nval bLookup = (x:Double, y:Double) => {\n    val feature:Option[Feature] = bFeatures.value.find(f => {\n        f.geometry.contains(new Point(x,y))\n    })\n    feature.map(f => {\n        f(\"borough\").convertTo[String]\n    }).getOrElse(\"NA\")\n}\nval boroughUDF = udf(bLookup)\n\ntaxiClean.groupBy(boroughUDF($\"dropoffX\", $\"dropoffY\")).count().show()\ntaxiClean.where(boroughUDF($\"dropoffX\", $\"dropoffY\") === \"NA\").show()\nval taxiDone = taxiClean.where(\"dropoffX != 0 and dropoffY != 0 and pickupX != 0 and pickupY != 0\").cache()\ntaxiDone.groupBy(boroughUDF($\"dropoffX\", $\"dropoffY\")).count().show()\n\nval sessions = taxiDone.repartition($\"license\").sortWithinPartitions($\"license\", $\"pickupTime\")\nsessions.cache()\n\ndef boroughDuration(t1: Trip, t2: Trip): (String, Long) = {\n    val b = bLookup(t1.dropoffX, t1.dropoffY)\n    val d = (t2.pickupTime - t1.dropoffTime) / 1000\n    (b,d)\n}\n\nval boroughDurations: DataFrame = sessions.mapPartitions(trips => {\n    val iter: Iterator[Seq[Trip]] = trips.sliding(2)\n    val viter = iter.filter(_.size == 2).filter(p => p(0).license == p(1).license)\n    viter.map(p => boroughDuration(p(0), p(1)))\n}).toDF(\"borough\", \"seconds\")\n\nboroughDurations.selectExpr(\"floor(seconds / 3600) as hours\").groupBy(\"hours\").count().sort(\"hours\").show()\nboroughDurations.where(\"seconds > 0 AND seconds < 60*60*4\").groupBy(\"borough\").agg(avg(\"seconds\"), stddev(\"seconds\")).show()\n\n\n","user":"anonymous","dateUpdated":"2018-05-28T14:34:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527256640560_1138762241","id":"20180525-135720_490938605","dateCreated":"2018-05-25T13:57:20+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2805"}],"name":"/EE_YU/Advanced_Analytics_with_Spark/chapter 8","id":"2DGCFCNEZ","angularObjects":{"2CDBRM16S:shared_process":[],"2CGD9PJEV:shared_process":[],"2CEXJP4VC:shared_process":[],"2CEWFDUFC:shared_process":[],"2CCUD92DR:shared_process":[],"2CE5CMKN8:shared_process":[],"2CCT6VFHB:shared_process":[],"2CGDHC2BR:shared_process":[],"2CD5H5KA3:shared_process":[],"2CCFV6EWC:shared_process":[],"2CG8AUTUC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}