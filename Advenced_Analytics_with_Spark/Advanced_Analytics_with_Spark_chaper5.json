{"paragraphs":[{"title":"Anomaly Detection in Network Traffic with K-means Clustering","text":"// data preparation\nval dataWithoutHeader = spark.read.option(\"inferSchema\",true).option(\"header\",false).csv(\"hdfs:///user/ds/kddcup.data\")\n\nval data = dataWithoutHeader.toDF(\n        \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\")\ndata.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)\n// clustering\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.Clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.VectorAssembler\n\nval numericOnly = data.drop(\"protocol_type\",\"service\",\"flag\").cache()\nval assembler = new VectorAssembler().\n    setInputCols(numericOnly.columns.filer(_ != \"label\")).\n    setOutputCols(\"featureVector\")\n    \nval kmeans = new KMeans().\n    setPredictionCol(\"cluster\").\n    setFeaturesCol(\"featureVector\")\n    \nval pipeline = new Pipeline().setStages(Array(assembler, kmeans))\nval pipelineModel = pipeline.fit(numericOnly)\nval kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n\nkmeansModel.clusterCenters.foreach(println)\n\nval withCluster = pipelineModel.tranform(numericOnly)\nwithCluster.select(\"cluster\",\"label\").\n    groupBy(\"cluster\",\"label\").count().\n    orderBy($\"cluster\",$\"count\".desc).\n    show(25)\n    \nimport org.apache.spark.sql.DataFrame\ndef clusteringScore0(data: DataFrame, k: Int): Double ={\n    val assembler = new VectorAssembler().\n        setInputCols(data.columns.filer(_ != \"label\")).\n        setOutputCol(\"featureVector\")\n    val kmeans = new KMeans().\n        setSeed(Random.nextLong()).\n        setK(k).\n        setPredictionCol(\"cluster\").\n        setFeaturesCol(\"featureVector\")\n        \n    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n    kmeansModel.computeCost(assembler.transform(data)) / data.count()\n}\n\n(20 to 100 by 20).map(k => (k, clusteringScore0(numericOnly,k))).foreach(println)","user":"anonymous","dateUpdated":"2018-05-18T15:04:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526637886293_-2103745272","id":"20180518-100446_161875160","dateCreated":"2018-05-18T10:04:46+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:232"},{"title":"Categorical Variables","text":"// translate categorical features into several binary indicator features\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\ndef oneHotPipeline(inputCol:String): (Pipeline, String) = {\n    val indexer = new StringIndexer().  // convert to integer\n        setInputCol(inputCol).\n        setOutputCol(inputCol + \"_indexed\")\n    val encoder = new OneHotEncoder().   // convert integer to binaries\n        setInputCol(inputCol + \"_indexed\").\n        setOutputCol(inputCol + \"_vec\")\n    val pipeline = new Pipeline().setStages(Array(indexer, encoder))\n    (pipeline, inputCol + \"_vec\")\n}","user":"anonymous","dateUpdated":"2018-05-22T08:57:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526974160695_-993432850","id":"20180522-072920_898030760","dateCreated":"2018-05-22T07:29:20+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"title":"Using labels with Entropy","text":"def entropy (counts: Iterable[Int]): Double = {\n    val values = counts.filter(_ > 0)\n    val n = values.map(_.toDouble).sum\n    values.map {v => \n    val p = v / n\n    -p * math.log(p)\n    }.sum\n}\n\nval clusterLabel = pipelineModel.transform(data).select(\"cluster\",\"label\").as[(Int, String)]\n\nval weightedClusterEntropy = clusterLabel.\n    groupByKey { case (cluster, _) => cluster}.mapGroups {case (_, clusterLabels) =>\n        val labels = clusterLables.map(case )_, label) => label}.toSeq\n        val labelCounts = labels.groupBy(identity).values.map(_.size)\n        labels.size * entropy(labelCounts)\n    }.collect()\n    \nweightedClusterEntropy.sum / data.count()","user":"anonymous","dateUpdated":"2018-05-22T10:28:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526979478637_-1337649837","id":"20180522-085758_2109735175","dateCreated":"2018-05-22T08:57:58+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"title":"clustering in action","text":"val pipelineModel = fitPipeline4(data, 180)\nval countByClusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\",\"label\")\ncountByClusterLabel.show()\n\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nval kMeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\nval centroids = kMeansModel.clusterCenters\nval clustered = pipelineModel.transform(data)\nval threshold = clustered.select(\"cluster\", \"scaledFeatureVector\").as[(Int, Vector)].map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec)}.orderBy($\"value\".desc).take(100).last\n\nval originalCols = data.columns\nval anomalies = clustered.filter {row =>\n    val cluster = row.getAs[Int](\"cluster\")\n    val vec = row.getAs[Vector](\"scaledFeatureVector\")\n    Vectors.sqdist(centroids(cluster), vec) >= threshold\n}.select(originalCols.head, originalCols.tail: _*)\n\nanomalies.first()","user":"anonymous","dateUpdated":"2018-05-22T10:39:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526984931937_-1425233098","id":"20180522-102851_1284034594","dateCreated":"2018-05-22T10:28:51+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:235"}],"name":"/EE_YU/Advanced_Analytics_with_Spark/chaper5","id":"2DG68GJYA","angularObjects":{"2CDBRM16S:shared_process":[],"2CGD9PJEV:shared_process":[],"2CEXJP4VC:shared_process":[],"2CEWFDUFC:shared_process":[],"2CCUD92DR:shared_process":[],"2CE5CMKN8:shared_process":[],"2CCT6VFHB:shared_process":[],"2CGDHC2BR:shared_process":[],"2CD5H5KA3:shared_process":[],"2CCFV6EWC:shared_process":[],"2CG8AUTUC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}