{"paragraphs":[{"title":"Estimating financial risk through Monte Carlo Simulation","text":"// preprocessing\n   // Yahoo data: Instrument\nimport java.time.LocalDate\nimport java.time.format.DateTimeFormatter\nval format = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n\nimport java.io.File\ndef readYahooHistory(file: File): Array[(LocalDate, Double)] = {\n    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n    val lines = scala.io.Source.fromFile(file).getLines().toSeq\n    lines.tail.map{line =>\n        val cols = line.split(',')\n        val date = LocalDate.parse(cols(0), formatter)\n        val value = cols(1).toDouble\n        (date, value)\n    }.reverse.toArray\n}\n\nval stockDir = new File(\"stocks/\")\nval files = stockDir.listFiles()\nval allStocks = files.iterator.flatMap{ file =>\n    try{\n        Some(readYahooHistory(file))\n    } catch{\n        case e: Exception => None\n    }\n}\nval rawStocks = allStocks.filer(_.size >= 260*5+10)\n    // factor file\nval factorsPrefix = \"factors/\"\nval rawFactors = Array(\"GSPC.csv\",\"IXIC.csv\",\"TYX.csv\",\"FVX.csv\").map(x => new File(factorsPrefix + x)).map(readYahooHistory)\n\n// trim the time series to the same region in time\nval start = LocalDate.of(2009,10,23)\nval end = LocalDate.of(2014,10,23)\n\ndef trimToRegion(history: Array[(LocalDate, Double)], start: LocalDate, end: LocalDate): Array[(LocalDate, Double)]={\n    var trimmed = history.dropWhile(_._1.isBefore(start)).takeWhile(x => x._1.isBefore(end) || x._1.isEqual(end))\n    if (trimmed.head._1 != end){\n        trimmed = trimmed ++ Array((end, trimmed.last._2))\n    }\n    trimmed\n}\n\nimport scala.collection.mutable.ArrayBuffer\ndef fillInHistory(history: Array[(DateTime, Double)], start: DateTime, end: DateTime): Array[(DateTime, Double)] ={\n    var cur = history\n    val filled = new ArrayBuffer[(DateTime, Double)]()\n    var curDate = start\n    while(curDate < end){\n        if (cur.tail.nonEmpty && cur.tail.head._1 == curDate){\n            cur = cur.tail\n        }\n        filled += ((curDate, cur.head._2))\n        \n        curDate += 1.days\n        // skip weekends\n        if (curDate.dayOfWeek().get > 5) curDate += 2.days\n\n    }\n    filled.toArray\n}\n\nval stocks = rawStocks.map(trimToRegion(_, start, end)).map(fillInHistory(_, start, end))\nval factors = (factors1 ++ factors2).map(trimToRegion(_, start, end)).map(fillInHistory(_, start, end))\n\n// to check if they have the same structure\n(stocks ++ factors).forall(_.size == stocks(0).size)","user":"anonymous","dateUpdated":"2018-05-30T12:32:37+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527577963269_1820386414","id":"20180529-071243_51901295","dateCreated":"2018-05-29T07:12:43+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5337"},{"title":"Determining the Factor Weights(coefficients)","text":" // the difference percent of two weeks (10 days)\ndef twoWeekReturns(history: Array[(LocalDate, Double)]):Array[Double] = {\n    history.sliding(10).   // use sliding to do transform time series of prices into an overlapping sequence of price movements over two-week intervals\n        map{ window => \n            val next = window.last._2\n            val prev = window.head._2\n            (next - prev)/prev\n        }.toArray\n}\n\nval stocksReturns = stocks.map(twoWeekReturns).toArray.toSeq\nval factorsReturns = factors.map(twoWeekReturns)\n\n// make factor matrix\ndef factorMatrix(histories: Seq[Array[Double]]): Array[Array[Double]] = {\n    val mat = new Array[Array[Double]](histories.head.length)\n    for (i <- histories.head.indices){\n        mat(i) = histories.map(_(i)).toArray\n    }\n    mat\n}\n\nval factorMat = factorMatrix(factorsReturns)\n\ndef featurize(factorReturns: Array[Double]): Array[Double]={\n    val squaredReturns = factorReturns.map(x => math.signum(x)*x*x)\n    val squreRootedReturns = factorReturns.map(x => math.signum(x)*math.sqrt(math.abs(x)))\n    squaredReturns ++ squareRootedReturns ++ factorReturns\n}\n\nval factorFeatures = factorMat.map(featurize)\n\nimport org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression\n\ndef linearModel(instrument:Array[Double], factorMatrix:Array[Array[Double]]): OLSMultipleLinearRegression = {\n    val regression = new OLSMultipleLinearRegression()\n    regression.newSampleData(instrument, factorMatrix)\n    regression\n}\n\nval factorWeights = stocksReturns.map(linearModel(_, factorFeatures)).map(_.estimateRegressionParameters()).toArray\n","user":"anonymous","dateUpdated":"2018-06-06T10:57:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527582986512_-2012033097","id":"20180529-083626_1725337709","dateCreated":"2018-05-29T08:36:26+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5338"},{"title":"Sampling","text":"// kernel density estimation\nimport org.apache.spark.mllib.stat.KernelDensity\nimport org.apache.spark.util.StatCounter\nimport breeze.plot._\n\ndef plotDistribution(samples: Array[Double]): Figure = {\n    val min = samples.min\n    val max = samples.max\n    val stddev = new StatCounter(samples).stdev\n    val bandwidth = 1.06 * stddev * math.pow(samples.size, -0.2)\n    \n    val domain = Range.Double(min, max, (max - min)/100).toList.toArray\n    val kd = new KernelDensity().setSample(samples.toSeq.toDS.rdd).setBandwidth(bandwidth)\n    val densities = kd.estimate(domain)\n    val f = Figure()\n    val p = f.subplot(0)\n    p += plot(domain, densities)\n    p.xlabel = \"Two Week Return ($)\" // x axis is factor value \n    p.ylabel = \"Density\"\n    f\n}\nplotDistribution(factorsReturns(0)) // for each factor, wiil have its own density plot\nplotDistribution(factorsReturns(2))\n\n// correlation between factors (Pearson's correlation estimation)\nimport org.apache.commons.math3.stat.correlation.PearsonsCorrelation\n\nval factorCor = new PearsonsCorrelation(factorMat).getCorrelationMatrix().getData()\nprintln(facotrCor.map(_.mkString(\"\\t\")).mkString(\"\\n\"))\n\n// Multivariate Normal Distribution\nimport org.apache.commons.math3.stat.correlation.Covariance\n\nval factorCov = new Covariance(factorMat).getCovarianceMatrix().getData()\nval factorMeans = factorsReturns.map(facotr => factor.sum / factor.size).toArray\n\nimport org.apache.commons.math3.distribution.MultivariateNormalDistribution\n\nval factorsDist = new MultivariateNormalDistribution(factorMeans, factorCov)\n// to sample a set of market conditions from it\nfactorDist.sample()","user":"anonymous","dateUpdated":"2018-06-06T12:18:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528271192370_279078315","id":"20180606-074632_1088359836","dateCreated":"2018-06-06T07:46:32+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5339"},{"title":"Running the Trials","text":"// partition-by-trials \n// we want different seed in each partition so that each partition generates different trials\nval parallelism = 1000\nval baseSeed = 1496\nval seeds = (baseSeed until baseSeed + parallelism)\nval seedDS = seeds.toDS(9.repartition(parallelism))\n\n// calcute the return of a single instrument under a single trial\ndef instrumentTrialReturn(instrument:Array[Double], trial:Array[Double]): Double = { //for parameter\"instrument\", it denotes the coefficients calculated by training, \"trial\" denotes the features of a single trial \n    var instrumentTrialReturn = instrument(0) // the first is the intercept term\n    var i = 0\n    while (i < trial.length){\n        instrumentTrialReturn += trial(i) * instrument(i+1)\n        i += 1\n    }\n    instrumentTrialReturn\n}\n\n// calculate the full return for a single trial, simply average them\ndef trialReturn(trial:Array[Double],instruments:Seq[Array[Double]]): Double = {\n    var totalReturn = 0.0\n    for (instrument <- instruments) {\n        totalReturn += instrumentTrialReturn(instrument, trial)\n    }\n    totalReturn/instruments.size\n}\n\n// all trials\nimport org.apache.commons.math3.random.MersenneTwister\n// for each trial, the features should be randomly different, here use seed and MersenneTwister to achive the randomness, and avoid generate repeating numbers\ndef trialReturns(seed:Long, numTrials:Int, instruments:Seq[Array[Double]], factorMeans:Array[Double], factorCovariances:Array[Array[Double]]): Seq[Double] = {\n    val rand = new MersenneTwister(seed)\n    val multivariateNormal = new MultivariateNormalDistribution(rand, factorMeans, factorCovariances)\n    val trialReturns = new Array[Double](numTrials)\n    \n    for (i <- 0 until numTrials){\n        val trialFactorRetruns = multivariateNormal.sample()\n        val trialFeatures = featurize(trialFactorReturns)\n        trialReturns(i) = trialReturn(trialFeatures, instruments)\n    }\n    trialReturns\n}\n\nval numTrials = 10000000 // the total number of trials\nval trials = seedDS.flatMap(trialReturns(_, numTrials/parallelism, factorWeights, factorMeans, factorCov))\n\ntrials.cache()\n\n// VaR: is the return of the best trial in the worst 5% of all trials\ndef fivePercentVaR(trials:Dataset[Double]): Double = {\n    val quantiles = trials.stat.approxQuantile(\"value\", Array(0.05), 0.0)\n    quantiles.head\n}\n\n// CVaR: the average return of the worst 5% of all trials\ndef fivePercentCVaR(trials:Dataset[Double]): Double = {\n    val topLosses = trials.orderBy(\"value\").limit(math.max(trials.count().toInt / 20, 1))\n    topLosses.agg(\"value\" -> \"avg\").first()(0).asInstanceOf[Double]\n}\n\nval conditionalValueAtRisk = fivePercentCVaR(trials)","user":"anonymous","dateUpdated":"2018-06-06T14:15:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528287531354_2140685753","id":"20180606-121851_216084890","dateCreated":"2018-06-06T12:18:51+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5340"},{"title":"Visualizing the Distribution of Returns","text":"import org.apache.spark.sql.functions\n\ndef plotDistribution(samples:Dataset[Double]): Figure = {\n    val (min, max, count, stddev) = samples.agg(\n            functions.min($\"value\"),\n            functions.max($\"value\"),\n            functions.count($\"value\"),\n            functions.stddev_pop($\"value\")).as[(Double, Double, Long, Double)].first()\n            \n    val bandwidth = 1.06 * stddev * math.pow(count, -0.2)\n    \n    // using toList before toArray avoids a Scala bug\n    val domain = Range.Double(min,max,(max-min)/100).toList.toArray\n    val kd = new KernelDensity().setSample(samples.rdd).setBandwidth(bandwidth)\n    val densities = kd.estimate(domain)\n    val f = Figure()\n    val p = f.subplot(0)\n    p += plot(domain, densities)\n    p.xlabel = \"Two Week Return ($)\"\n    p.ylabel = \"Density\"\n    f\n}\n\nplotDistribution(trials)","user":"anonymous","dateUpdated":"2018-06-07T08:18:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528291374822_-1491057114","id":"20180606-132254_211427447","dateCreated":"2018-06-06T13:22:54+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5341"},{"title":"Evaluating our Results","text":"// use bootstrapping to compute the confidence interval of our results\ndef bootstrappedConfidenceInterval(trials:Dataset[Double], computeStatistic:Dataset[Double]=>Double, numResamples:Int, probability:Double): (Double,Double) = {\n    val stats = (0 until numResamples).map{i => \n        val resample = trials.sample(true, 1.0)\n        computeStatistic(resample)\n    }.sorted\n    \n    val lowerIndex = (numResamples * probability / 2 - 1).toInt\n    val upperIndex = math.ceil(numResamples * (1 - probability / 2)).toInt\n    (stats(lowerIndex), stats(upperIndex))\n}\n\n// use proportion-of-failures(POF) on historical data to estimate how well our model matches reality\n// compare the mean of stock losses of each historical time intervals with VaR\nvar failures = 0\nfor (i <- stocksReturns.head.indices){\n    val loss = stocksReturns.map(_(i)).sum / stocksReturns.size\n    if (loss > valueAtRisk){\n        failtures += 1\n    }\n}\n\nval total = stocksReturns.size\nval confidenceLevel = 0.05\nval failtureRatio = failtures.toDouble / total\nval logNumer = ((total - failures)*math.log1p(-confidenceLevel)+failures*math.log(confidenceLevel))\nval logDenom = ((total - failures)*math.log1p(-failureRatio)+failures*math.log(failureRatio))\nval testStatistic = -2*(logNumer-logDenom)\n","user":"anonymous","dateUpdated":"2018-06-07T11:15:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528359491827_-532051671","id":"20180607-081811_1060607756","dateCreated":"2018-06-07T08:18:11+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5342"}],"name":"/EE_YU/Advanced_Analytics_with_Spark/chapter 9","id":"2DESTUZEV","angularObjects":{"2CDBRM16S:shared_process":[],"2CGD9PJEV:shared_process":[],"2CEXJP4VC:shared_process":[],"2CEWFDUFC:shared_process":[],"2CCUD92DR:shared_process":[],"2CE5CMKN8:shared_process":[],"2CCT6VFHB:shared_process":[],"2CGDHC2BR:shared_process":[],"2CD5H5KA3:shared_process":[],"2CCFV6EWC:shared_process":[],"2CG8AUTUC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}